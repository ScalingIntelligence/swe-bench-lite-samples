{
  "info": {
    "duration": 266.2388873100281,
    "submission": "diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py\nindex 1e05aba59..7d2c09006 100644\n--- a/sklearn/preprocessing/_encoders.py\n+++ b/sklearn/preprocessing/_encoders.py\n@@ -562,6 +562,10 @@ class OneHotEncoder(_BaseEncoder):\n \n         n_samples, n_features = X.shape\n \n+        # Ensure that the array is always cast to object type when strings are involved\n+        if np.issubdtype(X.dtype, np.str_) or X.dtype == np.object:\n+            X = X.astype(np.object)\n+\n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n         mask = X_mask.ravel()\n",
    "total_cost": 0.152343,
    "instance_id": "scikit-learn__scikit-learn-12471",
    "evaluation_name": "temperature_sweep_250_samples_50_problems_1.6"
  },
  "name": "AgenticLoop",
  "transitions": [
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The issue involves a specific problem with the OneHotEncoder when handling unknown categories and string labels. The ValueError occurs due to the truncation of long strings when transforming the data. The relevant code should be within the OneHotEncoder class and its associated methods, particularly those involving the 'handle_unknown' parameter and the transformation process. Given the detailed description, focus should be on the 'transform' method of OneHotEncoder and any related utility functions.",
            "search_requests": [
              {
                "query": "OneHotEncoder transform handle_unknown",
                "class_names": [
                  "OneHotEncoder"
                ],
                "code_snippet": null,
                "file_pattern": "sklearn/preprocessing/_encoders.py",
                "function_names": [
                  "transform"
                ]
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 228,
                "span_id": "OneHotEncoder._transform_new",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 1,
                "tokens": 182,
                "span_id": "OneHotEncoder.fit_transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 2,
                "tokens": 407,
                "span_id": "OneHotEncoder._legacy_transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 3,
                "tokens": 116,
                "span_id": "OneHotEncoder.transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 5,
                "tokens": 156,
                "span_id": "OneHotEncoder.fit",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 6,
                "tokens": 1480,
                "span_id": "OneHotEncoder",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 7,
                "tokens": 55,
                "span_id": "OneHotEncoder.active_features_",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 7,
                "tokens": 55,
                "span_id": "OneHotEncoder.n_values_",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 7,
                "tokens": 55,
                "span_id": "OneHotEncoder.feature_indices_",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 8,
                "tokens": 789,
                "span_id": "OneHotEncoder._handle_deprecations",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 9,
                "tokens": 554,
                "span_id": "OneHotEncoder._legacy_fit_transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 11,
                "tokens": 458,
                "span_id": "OneHotEncoder.inverse_transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 14,
                "tokens": 219,
                "span_id": "OneHotEncoder.get_feature_names",
                "file_path": "sklearn/preprocessing/_encoders.py"
              },
              {
                "rank": 15,
                "tokens": 83,
                "span_id": "OrdinalEncoder.transform",
                "file_path": "sklearn/preprocessing/_encoders.py"
              }
            ],
            "search_requests": [
              {
                "query": "OneHotEncoder transform handle_unknown",
                "class_names": [
                  "OneHotEncoder"
                ],
                "code_snippet": null,
                "file_pattern": "sklearn/preprocessing/_encoders.py",
                "function_names": [
                  "transform"
                ]
              }
            ]
          },
          "input_tokens": 3838,
          "output_tokens": 183,
          "retry_message": null,
          "completion_cost": 0.014259
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 228,
            "span_id": "OneHotEncoder._transform_new",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 1,
            "tokens": 182,
            "span_id": "OneHotEncoder.fit_transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 2,
            "tokens": 407,
            "span_id": "OneHotEncoder._legacy_transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 3,
            "tokens": 116,
            "span_id": "OneHotEncoder.transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 5,
            "tokens": 156,
            "span_id": "OneHotEncoder.fit",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 6,
            "tokens": 1480,
            "span_id": "OneHotEncoder",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 7,
            "tokens": 55,
            "span_id": "OneHotEncoder.active_features_",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 7,
            "tokens": 55,
            "span_id": "OneHotEncoder.n_values_",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 7,
            "tokens": 55,
            "span_id": "OneHotEncoder.feature_indices_",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 8,
            "tokens": 789,
            "span_id": "OneHotEncoder._handle_deprecations",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 9,
            "tokens": 554,
            "span_id": "OneHotEncoder._legacy_fit_transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 11,
            "tokens": 458,
            "span_id": "OneHotEncoder.inverse_transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 14,
            "tokens": 219,
            "span_id": "OneHotEncoder.get_feature_names",
            "file_path": "sklearn/preprocessing/_encoders.py"
          },
          {
            "rank": 15,
            "tokens": 83,
            "span_id": "OrdinalEncoder.transform",
            "file_path": "sklearn/preprocessing/_encoders.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "The issue reported pertains to the handling of unknown string entries in the OneHotEncoder when the 'handle_unknown' parameter is set to 'ignore'. Specifically, it mentions that when the first category alphabetically sorted is a long string and the array to be transformed contains small strings, this can lead to truncation and a ValueError. The relevant code spans should focus on the initialization, fitting, and transformation methods of the OneHotEncoder class, as well as any handling of the 'handle_unknown' parameter and string categories. I will identify the code spans that deal directly with these functionalities.",
            "identified_spans": [
              {
                "span_ids": [
                  "OneHotEncoder",
                  "OneHotEncoder.__init__",
                  "OneHotEncoder.fit",
                  "OneHotEncoder.fit_transform",
                  "OneHotEncoder.transform",
                  "OneHotEncoder._transform_new"
                ],
                "file_path": "sklearn/preprocessing/_encoders.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 4320,
          "output_tokens": 238,
          "retry_message": null,
          "completion_cost": 0.016530000000000003
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": true,
            "relevant": true,
            "scratch_pad": "The file context provided contains the main structure of the OneHotEncoder class including the methods fit, fit_transform, and transform. These methods are central to the handling of data transformation and fitting as described in the issue. The init method also includes the parameters relevant to the issue such as handle_unknown and categories. The specific error handling when using 'handle_unknown' as 'ignore' and the processing of data within the transform method align with the reported issue. Although not all internal functions like _fit and _transform are fully shown, they are implied to be part of the class and therefore relevant to the issue. Given the context provided, it is reasonable to conclude that the file context is complete for understanding the current state of the code concerning the reported issue.",
            "search_suggestions": null
          },
          "output": {},
          "input_tokens": 2667,
          "output_tokens": 192,
          "retry_message": null,
          "completion_cost": 0.010881
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "OneHotEncoder._transform_new",
            "file_path": "sklearn/preprocessing/_encoders.py",
            "scratch_pad": "The issue involves handling unknown categories when the categories are strings. Specifically, when an unknown string is encountered during transform, it is set to the first category in the sorted list of known categories, which can lead to truncation if the unknown string is shorter than this first category. To fix this, we need to ensure that the array can handle strings of varying lengths.",
            "instructions": "Update the _transform_new method to check the size of the elements in the array before transforming and cast them into objects if necessary to avoid truncation issues."
          },
          "output": {
            "span_id": "OneHotEncoder._transform_new",
            "end_line": 583,
            "file_path": "sklearn/preprocessing/_encoders.py",
            "start_line": 555,
            "instructions": "Update the _transform_new method to check the size of the elements in the array before transforming and cast them into objects if necessary to avoid truncation issues."
          },
          "input_tokens": 4891,
          "output_tokens": 168,
          "retry_message": null,
          "completion_cost": 0.017193
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "OneHotEncoder._transform_new",
        "end_line": 583,
        "file_path": "sklearn/preprocessing/_encoders.py",
        "max_tokens": 1000,
        "start_line": 555,
        "temperature": 1.6,
        "instructions": "Update the _transform_new method to check the size of the elements in the array before transforming and cast them into objects if necessary to avoid truncation issues.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "```xml\n<replace>\n    def _transform_new(self, X):\n        \"\"\"New implementation assuming categorical input\"\"\"\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n\n        # Check the size of elements and cast to object if necessary to avoid truncation\n        for i in range(n_features):\n            if X[:, i].dtype != np.object and np.any(np.char.str_len(X[:, i]) > 10):  # Placeholder for the actual size check logic\n                X[:, i] = X[:, i].astype(np.object)\n\n        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        feature_indices = np.cumsum(n_values)\n\n        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n        indptr = X_mask.sum(axis=1).cumsum()\n        indptr = np.insert(indptr, 0, 0)\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, feature_indices[-1]),\n                                dtype=self.dtype)\n        if not self.sparse:\n            return out.toarray()\n        else:\n            return out\n</replace>"
          },
          "output": {
            "diff": "--- sklearn/preprocessing/_encoders.py\n+++ sklearn/preprocessing/_encoders.py\n@@ -562,6 +562,11 @@\n \n         n_samples, n_features = X.shape\n \n+        # Check the size of elements and cast to object if necessary to avoid truncation\n+        for i in range(n_features):\n+            if X[:, i].dtype != np.object and np.any(np.char.str_len(X[:, i]) > 10):  # Placeholder for the actual size check logic\n+                X[:, i] = X[:, i].astype(np.object)\n+\n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n         mask = X_mask.ravel()\n",
            "message": "Applied the change to sklearn/preprocessing/_encoders.py.\n\n<scratch_pad>\n```xml\n</scratch_pad>",
            "verification_errors": [
              {
                "code": "F0002",
                "line": 1,
                "message": "/workspace/moatless/workdir/repos/112_repos/swe-bench_scikit-learn__scikit-learn/sklearn/preprocessing/_encoders.py: Fatal error while checking '/workspace/moatless/workdir/repos/112_repos/swe-bench_scikit-learn__scikit-learn/sklearn/preprocessing/_encoders.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/jordan/.cache/pylint/pylint-crash-2024-07-03-11-35-26.txt'.",
                "file_path": "sklearn/preprocessing/_encoders.py"
              }
            ]
          },
          "input_tokens": 6826,
          "output_tokens": 438,
          "retry_message": null,
          "completion_cost": 0.027048
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": "--- sklearn/preprocessing/_encoders.py\n+++ sklearn/preprocessing/_encoders.py\n@@ -562,6 +562,11 @@\n \n         n_samples, n_features = X.shape\n \n+        # Check the size of elements and cast to object if necessary to avoid truncation\n+        for i in range(n_features):\n+            if X[:, i].dtype != np.object and np.any(np.char.str_len(X[:, i]) > 10):  # Placeholder for the actual size check logic\n+                X[:, i] = X[:, i].astype(np.object)\n+\n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n         mask = X_mask.ravel()\n",
        "model": "deepseek-coder",
        "message": "Applied the change to sklearn/preprocessing/_encoders.py.\n\n<scratch_pad>\n```xml\n</scratch_pad>",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": [
          {
            "code": "F0002",
            "line": 1,
            "message": "/workspace/moatless/workdir/repos/112_repos/swe-bench_scikit-learn__scikit-learn/sklearn/preprocessing/_encoders.py: Fatal error while checking '/workspace/moatless/workdir/repos/112_repos/swe-bench_scikit-learn__scikit-learn/sklearn/preprocessing/_encoders.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/jordan/.cache/pylint/pylint-crash-2024-07-03-11-35-26.txt'.",
            "file_path": "sklearn/preprocessing/_encoders.py"
          }
        ],
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "OneHotEncoder._transform_new",
            "file_path": "sklearn/preprocessing/_encoders.py",
            "scratch_pad": "The lint error suggests a fatal error during the linting process, which is unusual. It might be due to an internal issue in the linting tool itself. Given the nature of the change and the context, it seems appropriate to attempt a simpler fix without introducing additional complexity.",
            "instructions": "Simplify the change by ensuring that the array is always cast to object type when strings are involved, without performing any size checks."
          },
          "output": {
            "span_id": "OneHotEncoder._transform_new",
            "end_line": 588,
            "file_path": "sklearn/preprocessing/_encoders.py",
            "start_line": 555,
            "instructions": "Simplify the change by ensuring that the array is always cast to object type when strings are involved, without performing any size checks."
          },
          "input_tokens": 5577,
          "output_tokens": 159,
          "retry_message": null,
          "completion_cost": 0.019116
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "OneHotEncoder._transform_new",
        "end_line": 588,
        "file_path": "sklearn/preprocessing/_encoders.py",
        "max_tokens": 1000,
        "start_line": 555,
        "temperature": 1.6,
        "instructions": "Simplify the change by ensuring that the array is always cast to object type when strings are involved, without performing any size checks.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "<replace>\n    def _transform_new(self, X):\n        \"\"\"New implementation assuming categorical input\"\"\"\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n\n        # Ensure that the array is always cast to object type when strings are involved\n        if np.issubdtype(X.dtype, np.str_) or X.dtype == np.object:\n            X = X.astype(np.object)\n\n        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        feature_indices = np.cumsum(n_values)\n\n        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n        indptr = X_mask.sum(axis=1).cumsum()\n        indptr = np.insert(indptr, 0, 0)\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, feature_indices[-1]),\n                                dtype=self.dtype)\n        if not self.sparse:\n            return out.toarray()\n        else:\n            return out\n</replace>"
          },
          "output": {
            "diff": "--- sklearn/preprocessing/_encoders.py\n+++ sklearn/preprocessing/_encoders.py\n@@ -562,10 +562,9 @@\n \n         n_samples, n_features = X.shape\n \n-        # Check the size of elements and cast to object if necessary to avoid truncation\n-        for i in range(n_features):\n-            if X[:, i].dtype != np.object and np.any(np.char.str_len(X[:, i]) > 10):  # Placeholder for the actual size check logic\n-                X[:, i] = X[:, i].astype(np.object)\n+        # Ensure that the array is always cast to object type when strings are involved\n+        if np.issubdtype(X.dtype, np.str_) or X.dtype == np.object:\n+            X = X.astype(np.object)\n \n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n",
            "message": "Applied the change to sklearn/preprocessing/_encoders.py.",
            "verification_errors": []
          },
          "input_tokens": 7003,
          "output_tokens": 401,
          "retry_message": null,
          "completion_cost": 0.027024
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": "--- sklearn/preprocessing/_encoders.py\n+++ sklearn/preprocessing/_encoders.py\n@@ -562,10 +562,9 @@\n \n         n_samples, n_features = X.shape\n \n-        # Check the size of elements and cast to object if necessary to avoid truncation\n-        for i in range(n_features):\n-            if X[:, i].dtype != np.object and np.any(np.char.str_len(X[:, i]) > 10):  # Placeholder for the actual size check logic\n-                X[:, i] = X[:, i].astype(np.object)\n+        # Ensure that the array is always cast to object type when strings are involved\n+        if np.issubdtype(X.dtype, np.str_) or X.dtype == np.object:\n+            X = X.astype(np.object)\n \n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n",
        "model": "deepseek-coder",
        "message": "Applied the change to sklearn/preprocessing/_encoders.py.",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": [],
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "finish",
            "finish": "The fix has been implemented by ensuring the array is cast to object type when strings are involved, which addresses the issue of string truncation without introducing additional complexity. The lint error was likely due to an internal issue with the linting tool, and the simplified fix should be appropriate.",
            "reject": null,
            "span_id": null,
            "file_path": null,
            "scratch_pad": "The change was simplified by ensuring the array is cast to object type when strings are involved, which avoids the need for size checks. This approach should handle the original issue of string truncation without introducing additional complexity.",
            "instructions": null
          },
          "output": {
            "message": "The fix has been implemented by ensuring the array is cast to object type when strings are involved, which addresses the issue of string truncation without introducing additional complexity. The lint error was likely due to an internal issue with the linting tool, and the simplified fix should be appropriate."
          },
          "input_tokens": 5974,
          "output_tokens": 158,
          "retry_message": null,
          "completion_cost": 0.020292
        }
      ]
    },
    {
      "name": "Finished",
      "state": {
        "model": "gpt-4o",
        "output": {
          "model": "deepseek-coder",
          "temperature": 1.6
        },
        "message": "The fix has been implemented by ensuring the array is cast to object type when strings are involved, which addresses the issue of string truncation without introducing additional complexity. The lint error was likely due to an internal issue with the linting tool, and the simplified fix should be appropriate.",
        "max_tokens": 1000,
        "temperature": 0.0,
        "max_iterations": null,
        "include_message_history": false
      },
      "actions": []
    }
  ],
  "initial_message": "OneHotEncoder ignore unknown error when categories are strings \n#### Description\r\n\r\nThis bug is very specific, but it happens when you set OneHotEncoder to ignore unknown entries.\r\nand your labels are strings. The memory of the arrays is not handled safely and it can lead to a ValueError\r\n\r\nBasically, when you call the transform method it will sets all the unknown strings on your array to OneHotEncoder.categories_[i][0] which is the first category alphabetically sorted given for fit\r\nIf this OneHotEncoder.categories_[i][0] is a long string, and the array that you want to transform has small strings, then it is impossible to fit the whole  OneHotEncoder.categories_[i][0] into the entries of the array we want to transform. So  OneHotEncoder.categories_[i][0]  is truncated and this raise the ValueError.\r\n\r\n\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\n\r\nimport numpy as np\r\nfrom sklearn.preprocessing import OneHotEncoder\r\n\r\n\r\n# It needs to be numpy arrays, the error does not appear \r\n# is you have lists of lists because it gets treated like an array of objects.\r\ntrain  = np.array([ '22','333','4444','11111111' ]).reshape((-1,1))\r\ntest   = np.array([ '55555',  '22' ]).reshape((-1,1))\r\n\r\nohe = OneHotEncoder(dtype=bool,handle_unknown='ignore')\r\n\r\nohe.fit( train )\r\nenc_test = ohe.transform( test )\r\n\r\n```\r\n\r\n\r\n#### Expected Results\r\nHere we should get an sparse matrix 2x4 false everywhere except at (1,1) the '22' that is known\r\n\r\n#### Actual Results\r\n\r\n> ValueError: y contains previously unseen labels: ['111111']\r\n\r\n\r\n#### Versions\r\nSystem:\r\n    python: 2.7.12 (default, Dec  4 2017, 14:50:18)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None\r\ncblas_libs: openblas, openblas\r\n  lib_dirs: /usr/lib\r\n\r\nPython deps:\r\n    Cython: 0.25.2\r\n     scipy: 0.18.1\r\nsetuptools: 36.7.0\r\n       pip: 9.0.1\r\n     numpy: 1.15.2\r\n    pandas: 0.19.1\r\n   sklearn: 0.21.dev0\r\n\r\n\r\n\r\n#### Comments\r\n\r\nI already implemented a fix for this issue, where I check the size of the elements in the array before, and I cast them into objects if necessary.\n"
}