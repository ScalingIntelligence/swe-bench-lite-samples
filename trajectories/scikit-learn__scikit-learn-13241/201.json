{
  "info": {
    "duration": 177.55990362167358,
    "submission": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..2e9481e3a 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -220,6 +220,9 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+        # Normalize eigenvectors to ensure consistent signs\n+        self.alphas_ = _deterministic_vector_sign_flip(self.alphas_)\n+\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n",
    "total_cost": 0.10243500000000001,
    "instance_id": "scikit-learn__scikit-learn-13241",
    "evaluation_name": "run_on_remaining_250_for_july_4_t_1.6"
  },
  "name": "AgenticLoop",
  "transitions": [
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The issue involves discrepancies in the signs of results produced by KernelPCA with an rbf kernel when run on the same dataset. This suggests an investigation into the eigenvalue or eigenvector computation process within KernelPCA. Relevant files include sklearn/decomposition/kernel_pca.py and possibly sklearn/utils/extmath.py for foundational linear algebra operations. Focus should be on any part of the code that handles or flips vector signs during transformation or computation.",
            "search_requests": [
              {
                "query": null,
                "class_names": [
                  "KernelPCA"
                ],
                "code_snippet": null,
                "file_pattern": "sklearn/decomposition/kernel_pca.py",
                "function_names": []
              },
              {
                "query": null,
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "sklearn/utils/extmath.py",
                "function_names": [
                  "svd_flip",
                  "_deterministic_vector_sign_flip"
                ]
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 1108,
                "span_id": "KernelPCA",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 192,
                "span_id": "KernelPCA.__init__",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 17,
                "span_id": "KernelPCA._pairwise",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 78,
                "span_id": "KernelPCA._get_kernel",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 359,
                "span_id": "KernelPCA._fit_transform",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 100,
                "span_id": "KernelPCA._fit_inverse_transform",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 172,
                "span_id": "KernelPCA.fit",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 133,
                "span_id": "KernelPCA.fit_transform",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 98,
                "span_id": "KernelPCA.transform",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 145,
                "span_id": "KernelPCA.inverse_transform",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "rank": 0,
                "tokens": 358,
                "span_id": "svd_flip",
                "file_path": "sklearn/utils/extmath.py"
              },
              {
                "rank": 0,
                "tokens": 139,
                "span_id": "_deterministic_vector_sign_flip",
                "file_path": "sklearn/utils/extmath.py"
              }
            ],
            "search_requests": [
              {
                "query": null,
                "class_names": [
                  "KernelPCA"
                ],
                "code_snippet": null,
                "file_pattern": "sklearn/decomposition/kernel_pca.py",
                "function_names": []
              },
              {
                "query": null,
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "sklearn/utils/extmath.py",
                "function_names": [
                  "svd_flip",
                  "_deterministic_vector_sign_flip"
                ]
              }
            ]
          },
          "input_tokens": 3670,
          "output_tokens": 208,
          "retry_message": null,
          "completion_cost": 0.01413
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 1108,
            "span_id": "KernelPCA",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 192,
            "span_id": "KernelPCA.__init__",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 17,
            "span_id": "KernelPCA._pairwise",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 78,
            "span_id": "KernelPCA._get_kernel",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 359,
            "span_id": "KernelPCA._fit_transform",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 100,
            "span_id": "KernelPCA._fit_inverse_transform",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 172,
            "span_id": "KernelPCA.fit",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 133,
            "span_id": "KernelPCA.fit_transform",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 98,
            "span_id": "KernelPCA.transform",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 145,
            "span_id": "KernelPCA.inverse_transform",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          },
          {
            "rank": 0,
            "tokens": 358,
            "span_id": "svd_flip",
            "file_path": "sklearn/utils/extmath.py"
          },
          {
            "rank": 0,
            "tokens": 139,
            "span_id": "_deterministic_vector_sign_flip",
            "file_path": "sklearn/utils/extmath.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "The reported issue pertains to variability in the signs of the results when using KernelPCA with the rbf kernel. The search results reveal essential components of KernelPCA's implementation, including initialization, kernel computation, and transformation methods. The specific area of interest is the transformation process where the kernel-based similarities are converted into principal components. Notably, the KernelPCA class utilizes eigenvalues and eigenvectors to generate the principal components, which are subject to sign differences due to the nature of eigendecomposition. Additionally, another file, sklearn/utils/extmath.py, contains utility functions for handling sign flips in singular value decomposition (SVD), which might also be relevant to the issue. Determining the exact cause of the sign flip requires understanding the mathematical properties of the eigendecomposition process and the post-processing steps to ensure consistent results. Therefore, the identified code spans focus on the KernelPCA class and its dependent functions in the sklearn library.",
            "identified_spans": [
              {
                "span_ids": [
                  "KernelPCA.__init__",
                  "KernelPCA._pairwise",
                  "KernelPCA._get_kernel",
                  "KernelPCA._fit_transform",
                  "KernelPCA._fit_inverse_transform",
                  "KernelPCA.fit",
                  "KernelPCA.fit_transform",
                  "KernelPCA.transform",
                  "KernelPCA.inverse_transform"
                ],
                "file_path": "sklearn/decomposition/kernel_pca.py"
              },
              {
                "span_ids": [
                  "svd_flip",
                  "_deterministic_vector_sign_flip"
                ],
                "file_path": "sklearn/utils/extmath.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 3362,
          "output_tokens": 396,
          "retry_message": null,
          "completion_cost": 0.016026
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "relevant": true,
            "scratch_pad": "The file context includes the `KernelPCA` class definition which is directly related to the issue, particularly the methods handling eigenvalue computations and transformations. It appears that the `_fit_transform` method is where the main computations take place. Additionally, the `sklearn/utils/extmath.py` file contains utility functions like `svd_flip` and `_deterministic_vector_sign_flip`, which could be involved in handling the sign of vectors. Given the information, it seems plausible that the discrepancies in signs might be addressed within these contexts.",
            "search_suggestions": "Investigate the usage and implications of the `svd_flip` function within the `KernelPCA` class or its related methods. Consider how the sign flipping might be occurring and if there are scenarios where it is not consistent. Also, review the documentation or comments within the `KernelPCA` class for any notes on sign indeterminacy."
          },
          "output": {
            "message": "Investigate the usage and implications of the `svd_flip` function within the `KernelPCA` class or its related methods. Consider how the sign flipping might be occurring and if there are scenarios where it is not consistent. Also, review the documentation or comments within the `KernelPCA` class for any notes on sign indeterminacy."
          },
          "input_tokens": 3136,
          "output_tokens": 221,
          "retry_message": null,
          "completion_cost": 0.012723
        }
      ]
    },
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": "Investigate the usage and implications of the `svd_flip` function within the `KernelPCA` class or its related methods. Consider how the sign flipping might be occurring and if there are scenarios where it is not consistent. Also, review the documentation or comments within the `KernelPCA` class for any notes on sign indeterminacy.",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": true,
            "scratch_pad": "The investigation into the usage of the `svd_flip` function within the `KernelPCA` class reveals that `svd_flip` is not directly called in the provided code snippets. However, the `KernelPCA` class uses eigenvalue decomposition to compute principal components, and the resulting eigenvectors can be subject to sign flipping due to their indeterminate nature. This indeterminacy is a common issue in eigenvector computation, where swapping the signs of any eigenvector does not affect the actual solution. In the context of `KernelPCA`, there does not appear to be any explicit handling or standardization of signs in the transformation process based on the provided code. Possible implications of this include the production of results with different signs between different runs or implementations, as observed in the issue description. Further review is recommended in any internals of `KernelPCA` or its dependencies that might implicitly handle sign flipping, to ensure consistent interpretation of principal components.",
            "search_requests": []
          },
          "output": {
            "message": "The investigation into the usage of the `svd_flip` function within the `KernelPCA` class reveals that `svd_flip` is not directly called in the provided code snippets. However, the `KernelPCA` class uses eigenvalue decomposition to compute principal components, and the resulting eigenvectors can be subject to sign flipping due to their indeterminate nature. This indeterminacy is a common issue in eigenvector computation, where swapping the signs of any eigenvector does not affect the actual solution. In the context of `KernelPCA`, there does not appear to be any explicit handling or standardization of signs in the transformation process based on the provided code. Possible implications of this include the production of results with different signs between different runs or implementations, as observed in the issue description. Further review is recommended in any internals of `KernelPCA` or its dependencies that might implicitly handle sign flipping, to ensure consistent interpretation of principal components."
          },
          "input_tokens": 5878,
          "output_tokens": 216,
          "retry_message": null,
          "completion_cost": 0.020874
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": "The investigation into the usage of the `svd_flip` function within the `KernelPCA` class reveals that `svd_flip` is not directly called in the provided code snippets. However, the `KernelPCA` class uses eigenvalue decomposition to compute principal components, and the resulting eigenvectors can be subject to sign flipping due to their indeterminate nature. This indeterminacy is a common issue in eigenvector computation, where swapping the signs of any eigenvector does not affect the actual solution. In the context of `KernelPCA`, there does not appear to be any explicit handling or standardization of signs in the transformation process based on the provided code. Possible implications of this include the production of results with different signs between different runs or implementations, as observed in the issue description. Further review is recommended in any internals of `KernelPCA` or its dependencies that might implicitly handle sign flipping, to ensure consistent interpretation of principal components.",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "KernelPCA._fit_transform",
            "file_path": "sklearn/decomposition/kernel_pca.py",
            "scratch_pad": "The issue regarding the sign flipping in KernelPCA results with an rbf kernel arises due to the indeterminate nature of eigenvectors computed via eigenvalue decomposition. To address this, we can normalize the eigenvectors after computation to enforce a consistent sign. This step is crucial for ensuring reproducibility of results across different runs or implementations. The proposed solution involves adding a normalization step within the `_fit_transform` method of the `KernelPCA` class to ensure that the signs of the eigenvectors are standardized, which should resolve the issue of different signs in the results.",
            "instructions": "Add a normalization step in the `_fit_transform` method of the `KernelPCA` class to ensure the eigenvectors have consistent signs. This can be achieved by applying a standardized method to flip the signs of the eigenvectors such that the largest value in the eigenvector is positive."
          },
          "output": {
            "span_id": "KernelPCA._fit_transform",
            "end_line": 223,
            "file_path": "sklearn/decomposition/kernel_pca.py",
            "start_line": 181,
            "instructions": "Add a normalization step in the `_fit_transform` method of the `KernelPCA` class to ensure the eigenvectors have consistent signs. This can be achieved by applying a standardized method to flip the signs of the eigenvectors such that the largest value in the eigenvector is positive."
          },
          "input_tokens": 3653,
          "output_tokens": 229,
          "retry_message": null,
          "completion_cost": 0.014394
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "KernelPCA._fit_transform",
        "end_line": 223,
        "file_path": "sklearn/decomposition/kernel_pca.py",
        "max_tokens": 1000,
        "start_line": 181,
        "temperature": 1.6,
        "instructions": "Add a normalization step in the `_fit_transform` method of the `KernelPCA` class to ensure the eigenvectors have consistent signs. This can be achieved by applying a standardized method to flip the signs of the eigenvectors such that the largest value in the eigenvector is positive.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "```xml\n<replace>\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        # Normalize eigenvectors to ensure consistent signs\n        self.alphas_ = _deterministic_vector_sign_flip(self.alphas_)\n\n        return K\n</replace>"
          },
          "output": {
            "diff": "--- sklearn/decomposition/kernel_pca.py\n+++ sklearn/decomposition/kernel_pca.py\n@@ -220,6 +220,9 @@\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+        # Normalize eigenvectors to ensure consistent signs\n+        self.alphas_ = _deterministic_vector_sign_flip(self.alphas_)\n+\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n",
            "message": "Applied the change to sklearn/decomposition/kernel_pca.py.\n\n<scratch_pad>\n```xml\n</scratch_pad>",
            "verification_errors": [
              {
                "code": "E0602",
                "line": 224,
                "message": "Undefined variable '_deterministic_vector_sign_flip'",
                "file_path": "sklearn/decomposition/kernel_pca.py"
              }
            ]
          },
          "input_tokens": 5456,
          "output_tokens": 528,
          "retry_message": null,
          "completion_cost": 0.024288
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": "--- sklearn/decomposition/kernel_pca.py\n+++ sklearn/decomposition/kernel_pca.py\n@@ -220,6 +220,9 @@\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+        # Normalize eigenvectors to ensure consistent signs\n+        self.alphas_ = _deterministic_vector_sign_flip(self.alphas_)\n+\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n",
        "model": "deepseek-coder",
        "message": "Applied the change to sklearn/decomposition/kernel_pca.py.\n\n<scratch_pad>\n```xml\n</scratch_pad>",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": [
          {
            "code": "E0602",
            "line": 224,
            "message": "Undefined variable '_deterministic_vector_sign_flip'",
            "file_path": "sklearn/decomposition/kernel_pca.py"
          }
        ],
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "imports",
            "file_path": "sklearn/decomposition/kernel_pca.py",
            "scratch_pad": "The previous diff introduced a lint error because the `_deterministic_vector_sign_flip` function was not defined in the `KernelPCA` class file. This function is actually defined in the `extmath.py` file, so we need to import it at the beginning of the `kernel_pca.py` file to resolve this lint error.",
            "instructions": "Import the `_deterministic_vector_sign_flip` function from the `sklearn.utils.extmath` module at the beginning of the `sklearn/decomposition/kernel_pca.py` file."
          },
          "output": null,
          "input_tokens": 4167,
          "output_tokens": 192,
          "retry_message": "Span not found: imports. Available spans: KernelPCA._fit_transform, KernelPCA._fit_inverse_transform, KernelPCA._get_kernel, KernelPCA.fit, KernelPCA.transform, KernelPCA.inverse_transform, KernelPCA.fit_transform, KernelPCA.__init__, KernelPCA._pairwise, KernelPCA, docstring",
          "completion_cost": 0.015381
        },
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "initial_imports",
            "file_path": "sklearn/decomposition/kernel_pca.py",
            "scratch_pad": "The intended span 'imports' for adding the import statement was not found. According to the provided spans, the best place to include the necessary import statement is at the beginning of the `kernel_pca.py` file, before the definition of the `KernelPCA` class and the other code that follows it. This will ensure that the `_deterministic_vector_sign_flip` function is available for use within the `KernelPCA` class methods.",
            "instructions": "Add the import statement for `_deterministic_vector_sign_flip` from `sklearn.utils.extmath` at the beginning of the `kernel_pca.py` file, right after the existing imports."
          },
          "output": null,
          "input_tokens": 4438,
          "output_tokens": 216,
          "retry_message": "Span not found: initial_imports. Available spans: KernelPCA._fit_transform, KernelPCA._fit_inverse_transform, KernelPCA._get_kernel, KernelPCA.fit, KernelPCA.transform, KernelPCA.inverse_transform, KernelPCA.fit_transform, KernelPCA.__init__, KernelPCA._pairwise, KernelPCA, docstring",
          "completion_cost": 0.016554000000000003
        },
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "start_of_file",
            "file_path": "sklearn/decomposition/kernel_pca.py",
            "scratch_pad": "Since the span 'initial_imports' was not found, it seems the best alternative is to add the import statement for `_deterministic_vector_sign_flip` from `sklearn.utils.extmath` at the very beginning of the file, right after the existing import statements. This ensures that the function is available for use in the `KernelPCA` class methods.",
            "instructions": "Add the following import statement at the beginning of the `kernel_pca.py` file, right after the existing imports: `from sklearn.utils.extmath import _deterministic_vector_sign_flip`."
          },
          "output": null,
          "input_tokens": 4736,
          "output_tokens": 201,
          "retry_message": "Span not found: start_of_file. Available spans: KernelPCA._fit_transform, KernelPCA._fit_inverse_transform, KernelPCA._get_kernel, KernelPCA.fit, KernelPCA.transform, KernelPCA.inverse_transform, KernelPCA.fit_transform, KernelPCA.__init__, KernelPCA._pairwise, KernelPCA, docstring",
          "completion_cost": 0.017223
        }
      ]
    }
  ],
  "initial_message": "Differences among the results of KernelPCA with rbf kernel\nHi there,\r\nI met with a problem:\r\n\r\n#### Description\r\nWhen I run KernelPCA for dimension reduction for the same datasets, the results are different in signs.\r\n\r\n#### Steps/Code to Reproduce\r\nJust to reduce the dimension to 7 with rbf kernel:\r\npca = KernelPCA(n_components=7, kernel='rbf', copy_X=False, n_jobs=-1)\r\npca.fit_transform(X)\r\n\r\n#### Expected Results\r\nThe same result.\r\n\r\n#### Actual Results\r\nThe results are the same except for their signs:(\r\n[[-0.44457617 -0.18155886 -0.10873474  0.13548386 -0.1437174  -0.057469\t0.18124364]] \r\n\r\n[[ 0.44457617  0.18155886  0.10873474 -0.13548386 -0.1437174  -0.057469 -0.18124364]] \r\n\r\n[[-0.44457617 -0.18155886  0.10873474  0.13548386  0.1437174   0.057469  0.18124364]] \r\n\r\n#### Versions\r\n0.18.1\r\n\n"
}