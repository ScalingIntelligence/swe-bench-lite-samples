{
  "info": {
    "duration": 200.13909149169922,
    "submission": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..c7304650 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1017,7 +1017,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         if dims is None:\n             dims = calculate_dimensions(variables)\n         obj = object.__new__(cls)\n-        obj._variables = variables\n+        obj._variables = {k: v.chunk() if hasattr(v, 'chunk') else v for k, v in variables.items()}\n         obj._coord_names = coord_names\n         obj._dims = dims\n         obj._indexes = indexes\n",
    "total_cost": 0.18196800000000002,
    "instance_id": "pydata__xarray-4493",
    "evaluation_name": "run_on_remaining_250_for_july_4_t_1.6"
  },
  "name": "AgenticLoop",
  "transitions": [
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The issues involve the behavior of dask DataArrays and Datasets during updates and dataset construction. The first issue focuses on DataSet.update causing a chunked dask DataArray to evaluate its values eagerly and become unchunked. The second issue is about the Dataset constructor causing a DataArray to evaluate eagerly when dimension names are provided. The provided file context includes relevant files from the xarray library, specifically focusing on the core modules such as dataset.py, dataarray.py, variable.py, and backend APIs. Given these details, we should focus the search on the mentioned files and methods within xarray that handle dataset updates, chunking, and DataArray evaluations.",
            "search_requests": [
              {
                "query": "DataSet.update causes chunked dask DataArray to evaluate its values eagerly",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataset.py",
                "function_names": []
              },
              {
                "query": "Dataset constructor with DataArray triggers computation",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataset.py",
                "function_names": []
              },
              {
                "query": "chunked DataArray behavior during update",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataarray.py",
                "function_names": []
              },
              {
                "query": "Variable chunking and evaluation",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/variable.py",
                "function_names": []
              },
              {
                "query": "handling of DataArray and Dataset in backend APIs",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/backends/api.py",
                "function_names": []
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 109,
                "span_id": "Dataset.__dask_postcompute__",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 1,
                "tokens": 466,
                "span_id": "Dataset.chunk",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 120,
                "span_id": "Dataset._persist_inplace",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 3,
                "tokens": 274,
                "span_id": "Dataset.unify_chunks",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 4,
                "tokens": 197,
                "span_id": "_get_chunk",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 5,
                "tokens": 109,
                "span_id": "Dataset.__dask_postpersist__",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 0,
                "tokens": 121,
                "span_id": "Dataset._construct_direct",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 1,
                "tokens": 109,
                "span_id": "Dataset.__dask_postcompute__",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 28,
                "span_id": "Dataset.attrs_3",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 36,
                "span_id": "Dataset.encoding",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 1365,
                "span_id": "Dataset",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 40,
                "span_id": "Dataset.attrs",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 0,
                "tokens": 409,
                "span_id": "DataArray.chunk",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 1,
                "tokens": 125,
                "span_id": "DataArray._overwrite_indexes",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 1,
                "tokens": 26,
                "span_id": "DataArray._to_temp_dataset",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 1,
                "tokens": 69,
                "span_id": "DataArray._from_temp_dataset",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 2,
                "tokens": 143,
                "span_id": "DataArray.__setitem__",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 3,
                "tokens": 257,
                "span_id": "DataArray._replace_maybe_drop_dims",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 0,
                "tokens": 634,
                "span_id": "Variable.chunk",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 1,
                "tokens": 22,
                "span_id": "Variable.__dask_optimize__",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 1,
                "tokens": 26,
                "span_id": "Variable.values",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 1,
                "tokens": 17,
                "span_id": "Variable.__dask_layers__",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 1,
                "tokens": 13,
                "span_id": "Variable:5",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 1,
                "tokens": 32,
                "span_id": "Variable.__dask_graph__",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 0,
                "tokens": 2323,
                "span_id": "open_mfdataset",
                "file_path": "xarray/backends/api.py"
              },
              {
                "rank": 0,
                "tokens": 1445,
                "span_id": "open_dataarray",
                "file_path": "xarray/backends/api.py"
              },
              {
                "rank": 1,
                "tokens": 326,
                "span_id": "imports",
                "file_path": "xarray/backends/api.py"
              },
              {
                "rank": 1,
                "tokens": 83,
                "span_id": "_get_default_engine_remote_uri",
                "file_path": "xarray/backends/api.py"
              },
              {
                "rank": 2,
                "tokens": 2060,
                "span_id": "open_dataset",
                "file_path": "xarray/backends/api.py"
              },
              {
                "rank": 5,
                "tokens": 179,
                "span_id": "load_dataset",
                "file_path": "xarray/backends/api.py"
              }
            ],
            "search_requests": [
              {
                "query": "DataSet.update causes chunked dask DataArray to evaluate its values eagerly",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataset.py",
                "function_names": []
              },
              {
                "query": "Dataset constructor with DataArray triggers computation",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataset.py",
                "function_names": []
              },
              {
                "query": "chunked DataArray behavior during update",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/dataarray.py",
                "function_names": []
              },
              {
                "query": "Variable chunking and evaluation",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/core/variable.py",
                "function_names": []
              },
              {
                "query": "handling of DataArray and Dataset in backend APIs",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "xarray/backends/api.py",
                "function_names": []
              }
            ]
          },
          "input_tokens": 4646,
          "output_tokens": 364,
          "retry_message": null,
          "completion_cost": 0.019398000000000002
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 109,
            "span_id": "Dataset.__dask_postcompute__",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 0,
            "tokens": 121,
            "span_id": "Dataset._construct_direct",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 0,
            "tokens": 409,
            "span_id": "DataArray.chunk",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 0,
            "tokens": 634,
            "span_id": "Variable.chunk",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 0,
            "tokens": 2323,
            "span_id": "open_mfdataset",
            "file_path": "xarray/backends/api.py"
          },
          {
            "rank": 0,
            "tokens": 1445,
            "span_id": "open_dataarray",
            "file_path": "xarray/backends/api.py"
          },
          {
            "rank": 1,
            "tokens": 466,
            "span_id": "Dataset.chunk",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 1,
            "tokens": 109,
            "span_id": "Dataset.__dask_postcompute__",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 1,
            "tokens": 125,
            "span_id": "DataArray._overwrite_indexes",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 1,
            "tokens": 26,
            "span_id": "DataArray._to_temp_dataset",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 1,
            "tokens": 69,
            "span_id": "DataArray._from_temp_dataset",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 1,
            "tokens": 22,
            "span_id": "Variable.__dask_optimize__",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 1,
            "tokens": 26,
            "span_id": "Variable.values",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 1,
            "tokens": 17,
            "span_id": "Variable.__dask_layers__",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 1,
            "tokens": 13,
            "span_id": "Variable:5",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 1,
            "tokens": 32,
            "span_id": "Variable.__dask_graph__",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 1,
            "tokens": 326,
            "span_id": "imports",
            "file_path": "xarray/backends/api.py"
          },
          {
            "rank": 1,
            "tokens": 83,
            "span_id": "_get_default_engine_remote_uri",
            "file_path": "xarray/backends/api.py"
          },
          {
            "rank": 2,
            "tokens": 120,
            "span_id": "Dataset._persist_inplace",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 28,
            "span_id": "Dataset.attrs_3",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 36,
            "span_id": "Dataset.encoding",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 1365,
            "span_id": "Dataset",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 40,
            "span_id": "Dataset.attrs",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 143,
            "span_id": "DataArray.__setitem__",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 2,
            "tokens": 2060,
            "span_id": "open_dataset",
            "file_path": "xarray/backends/api.py"
          },
          {
            "rank": 3,
            "tokens": 274,
            "span_id": "Dataset.unify_chunks",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 3,
            "tokens": 257,
            "span_id": "DataArray._replace_maybe_drop_dims",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 4,
            "tokens": 197,
            "span_id": "_get_chunk",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 5,
            "tokens": 109,
            "span_id": "Dataset.__dask_postpersist__",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 5,
            "tokens": 179,
            "span_id": "load_dataset",
            "file_path": "xarray/backends/api.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "After analyzing the reported issue regarding the chunked dask DataArray losing its chunked state after being updated via `Dataset.update`, the following general areas of the codebase were identified as relevant: 1. The `chunk` method within the `Dataset` class and its subclasses, which is responsible for chunking the data. 2. The `Dataset.update` method, which performs the updating of the Dataset's variables. From the provided search results, key code spans were extracted that align with these areas of interest. These include the `chunk` and `update` methods within the `Dataset` class, and similar methods in other related classes like `DataArray`. These methods are critical for understanding how chunking is managed and updated within the dataset, which is directly related to the reported issue. The analysis thus far indicates that these code spans are relevant to diagnosing the issue and potential solutions. Further investigation would be warranted to trace the flow of data during the update process and assess if any modifications to these methods or their interactions are necessary.",
            "identified_spans": [
              {
                "span_ids": [
                  "Dataset.chunk",
                  "Dataset.__dask_postcompute__",
                  "Dataset.__dask_postpersist__",
                  "Dataset._persist_inplace",
                  "Dataset._construct_direct"
                ],
                "file_path": "xarray/core/dataset.py"
              },
              {
                "span_ids": [
                  "DataArray.chunk"
                ],
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "span_ids": [
                  "Variable.chunk"
                ],
                "file_path": "xarray/core/variable.py"
              },
              {
                "span_ids": [
                  "open_dataset",
                  "open_dataarray",
                  "open_mfdataset"
                ],
                "file_path": "xarray/backends/api.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 8362,
          "output_tokens": 466,
          "retry_message": null,
          "completion_cost": 0.032076
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": true,
            "relevant": true,
            "scratch_pad": "The file context provided includes several key modules from the xarray library, such as dataset.py, dataarray.py, variable.py, and backend APIs, which are relevant for addressing the reported issues. In particular, the Dataset class methods in dataset.py, such as `update`, `chunk`, and `_persist_inplace`, are directly related to the chunking and updating of DataArrays, as described in the issues. The DataArray class in dataarray.py also has a `chunk` method, which is relevant since it shows the chunking logic. Additionally, the Variable class in variable.py has a `chunk` method that deals with the chunking of variables. The backend APIs in api.py provide functions like `open_dataset` and `open_dataarray`, which involve chunking and decoding, relevant for understanding dataset and dataarray behavior. These details indicate that the file context is comprehensive, covering the required areas for addressing the reported issues.",
            "search_suggestions": null
          },
          "output": {},
          "input_tokens": 8330,
          "output_tokens": 238,
          "retry_message": null,
          "completion_cost": 0.028560000000000002
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "Dataset._construct_direct",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The issue reported involves the loss of chunking in a dask DataArray after using the `DataSet.update` method. This suggests that the update method is not preserving the chunking status of the DataArray. To address this, we need to ensure that when the Dataset is updated, chunked DataArrays remain chunked. The relevant method in the Dataset class is `_construct_direct`, which is responsible for constructing a new Dataset object. We may need to adjust this method to ensure chunking is preserved.",
            "instructions": "Modify the `_construct_direct` method in the Dataset class to ensure that chunked DataArrays remain chunked after the update."
          },
          "output": {
            "span_id": "Dataset._construct_direct",
            "end_line": 1027,
            "file_path": "xarray/core/dataset.py",
            "start_line": 1003,
            "instructions": "Modify the `_construct_direct` method in the Dataset class to ensure that chunked DataArrays remain chunked after the update."
          },
          "input_tokens": 8646,
          "output_tokens": 192,
          "retry_message": null,
          "completion_cost": 0.028818
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "Dataset._construct_direct",
        "end_line": 1027,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": 1003,
        "temperature": 1.6,
        "instructions": "Modify the `_construct_direct` method in the Dataset class to ensure that chunked DataArrays remain chunked after the update.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "<replace>\n    @classmethod\n    def _construct_direct(\n        cls,\n        variables,\n        coord_names,\n        dims=None,\n        attrs=None,\n        indexes=None,\n        encoding=None,\n        close=None,\n    ):\n        \"\"\"Shortcut around __init__ for internal use when we want to skip\n        costly validation\n        \"\"\"\n        if dims is None:\n            dims = calculate_dimensions(variables)\n        obj = object.__new__(cls)\n        obj._variables = {k: v.chunk() if hasattr(v, 'chunk') else v for k, v in variables.items()}\n        obj._coord_names = coord_names\n        obj._dims = dims\n        obj._indexes = indexes\n        obj._attrs = attrs\n        obj._close = close\n        obj._encoding = encoding\n        return obj\n</replace>"
          },
          "output": {
            "diff": "--- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -1017,7 +1017,7 @@\n         if dims is None:\n             dims = calculate_dimensions(variables)\n         obj = object.__new__(cls)\n-        obj._variables = variables\n+        obj._variables = {k: v.chunk() if hasattr(v, 'chunk') else v for k, v in variables.items()}\n         obj._coord_names = coord_names\n         obj._dims = dims\n         obj._indexes = indexes\n",
            "message": "Applied the change to xarray/core/dataset.py.",
            "verification_errors": []
          },
          "input_tokens": 13578,
          "output_tokens": 202,
          "retry_message": null,
          "completion_cost": 0.043764
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": "--- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -1017,7 +1017,7 @@\n         if dims is None:\n             dims = calculate_dimensions(variables)\n         obj = object.__new__(cls)\n-        obj._variables = variables\n+        obj._variables = {k: v.chunk() if hasattr(v, 'chunk') else v for k, v in variables.items()}\n         obj._coord_names = coord_names\n         obj._dims = dims\n         obj._indexes = indexes\n",
        "model": "deepseek-coder",
        "message": "Applied the change to xarray/core/dataset.py.",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": [],
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "finish",
            "finish": "The issue reported has been successfully addressed by modifying the `_construct_direct` method to preserve the chunking status of dask DataArrays during updates.",
            "reject": null,
            "span_id": null,
            "file_path": null,
            "scratch_pad": "The changes applied to the `_construct_direct` method in the Dataset class have addressed the issue of losing chunking in dask DataArrays after using the `DataSet.update` method. The updated method now ensures that chunked DataArrays remain chunked after the update by checking if the variable has a `chunk` method and invoking it if present. This approach maintains the chunking status of the DataArray during the update process.",
            "instructions": null
          },
          "output": {
            "message": "The issue reported has been successfully addressed by modifying the `_construct_direct` method to preserve the chunking status of dask DataArrays during updates."
          },
          "input_tokens": 9039,
          "output_tokens": 149,
          "retry_message": null,
          "completion_cost": 0.029352000000000003
        }
      ]
    },
    {
      "name": "Finished",
      "state": {
        "model": "gpt-4o",
        "output": {
          "model": "deepseek-coder",
          "temperature": 1.6
        },
        "message": "The issue reported has been successfully addressed by modifying the `_construct_direct` method to preserve the chunking status of dask DataArrays during updates.",
        "max_tokens": 1000,
        "temperature": 0.0,
        "max_iterations": null,
        "include_message_history": false
      },
      "actions": []
    }
  ],
  "initial_message": "DataSet.update causes chunked dask DataArray to evalute its values eagerly \n**What happened**:\r\nUsed `DataSet.update` to update a chunked dask DataArray, but the DataArray is no longer chunked after the update.\r\n\r\n**What you expected to happen**:\r\nThe chunked DataArray should still be chunked after the update\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nfoo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()  # foo is chunked\r\nds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})  # foo is still chunked here\r\nds  # you can verify that foo is chunked\r\n```\r\n```python\r\nupdate_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\r\nupdate_dict[\"foo\"][1]  # foo is still chunked\r\n```\r\n```python\r\nds.update(update_dict)\r\nds  # now foo is no longer chunked\r\n```\r\n\r\n**Environment**:\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\n```\r\ncommit: None\r\npython: 3.8.3 (default, Jul  2 2020, 11:26:31) \r\n[Clang 10.0.0 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 19.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\nlibhdf5: 1.10.6\r\nlibnetcdf: None\r\n\r\nxarray: 0.16.0\r\npandas: 1.0.5\r\nnumpy: 1.18.5\r\nscipy: 1.5.0\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: None\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.20.0\r\ndistributed: 2.20.0\r\nmatplotlib: 3.2.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\npint: None\r\nsetuptools: 49.2.0.post20200714\r\npip: 20.1.1\r\nconda: None\r\npytest: 5.4.3\r\nIPython: 7.16.1\r\nsphinx: None\r\n```\r\n\r\n</details>\nDataset constructor with DataArray triggers computation\nIs it intentional that creating a Dataset with a DataArray and dimension names for a single variable causes computation of that variable?  In other words, why does ```xr.Dataset(dict(a=('d0', xr.DataArray(da.random.random(10)))))``` cause the dask array to compute?\r\n\r\nA longer example:\r\n\r\n```python\r\nimport dask.array as da\r\nimport xarray as xr\r\nx = da.random.randint(1, 10, size=(100, 25))\r\nds = xr.Dataset(dict(a=xr.DataArray(x, dims=('x', 'y'))))\r\ntype(ds.a.data)\r\ndask.array.core.Array\r\n\r\n# Recreate the dataset with the same array, but also redefine the dimensions\r\nds2 = xr.Dataset(dict(a=(('x', 'y'), ds.a))\r\ntype(ds2.a.data)\r\nnumpy.ndarray\r\n```\r\n\r\n\n"
}