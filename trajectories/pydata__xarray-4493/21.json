{
  "info": {
    "duration": 198.58498311042786,
    "submission": "",
    "total_cost": 0.287091,
    "instance_id": "pydata__xarray-4493",
    "evaluation_name": "run_on_remaining_250_for_july_4_t_1.6"
  },
  "name": "AgenticLoop",
  "transitions": [
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The issue involves the behavior of chunked dask DataArrays when using `DataSet.update` and the Dataset constructor with DataArrays. Both operations seem to cause the chunked DataArray to evaluate its values eagerly, which is not the expected behavior. The goal is to investigate the `DataSet.update` method and the Dataset constructor in `xarray` to understand why the chunked DataArray is losing its chunking properties after these operations. Specifically, we need to focus on the `chunk` method within the `Dataset` and `DataArray` classes to ensure that chunking is maintained after updates and construction.",
            "search_requests": [
              {
                "query": null,
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "**/xarray/**/*.py",
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 409,
                "span_id": "DataArray.chunk",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 1,
                "tokens": 466,
                "span_id": "Dataset.chunk",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 1,
                "tokens": 170,
                "span_id": "Dataset.update",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 2,
                "tokens": 634,
                "span_id": "Variable.chunk",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 2,
                "tokens": 37,
                "span_id": "IndexVariable.chunk",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 3,
                "tokens": 62,
                "span_id": "Coordinates.update",
                "file_path": "xarray/core/coordinates.py"
              },
              {
                "rank": 4,
                "tokens": 24,
                "span_id": "OrderedSet.update",
                "file_path": "xarray/core/utils.py"
              }
            ],
            "search_requests": [
              {
                "query": null,
                "class_names": [],
                "code_snippet": null,
                "file_pattern": "**/xarray/**/*.py",
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "input_tokens": 4646,
          "output_tokens": 191,
          "retry_message": null,
          "completion_cost": 0.016803000000000002
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 409,
            "span_id": "DataArray.chunk",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 1,
            "tokens": 466,
            "span_id": "Dataset.chunk",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 1,
            "tokens": 170,
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 2,
            "tokens": 634,
            "span_id": "Variable.chunk",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 2,
            "tokens": 37,
            "span_id": "IndexVariable.chunk",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 3,
            "tokens": 62,
            "span_id": "Coordinates.update",
            "file_path": "xarray/core/coordinates.py"
          },
          {
            "rank": 4,
            "tokens": 24,
            "span_id": "OrderedSet.update",
            "file_path": "xarray/core/utils.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "To address the reported issue, we need to identify where the Dataset.update method is causing the chunked dask DataArray to evaluate its values eagerly. This means we are looking for the update method implementation within the Dataset class and understanding how it interacts with other methods, particularly those related to chunking. Additionally, we need to ensure that the chunking behavior remains consistent after the update operation. The provided search results include implementations of the DataArray and Dataset classes with methods like chunk and update. Notably, the Dataset class's update method delegates the merge process to the dataset_update_method, which could potentially trigger computations. Also, we see how the chunk method is implemented in both Dataset and Variable classes to manage the chunked data. Analyzing these methods and understanding their interactions will help us determine the relevant code spans.",
            "identified_spans": [
              {
                "span_ids": [
                  "DataArray",
                  "DataArray.__init__",
                  "DataArray.chunk"
                ],
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "span_ids": [
                  "Dataset",
                  "Dataset.chunk",
                  "Dataset.update"
                ],
                "file_path": "xarray/core/dataset.py"
              },
              {
                "span_ids": [
                  "Variable",
                  "Variable.chunk",
                  "IndexVariable",
                  "IndexVariable.chunk"
                ],
                "file_path": "xarray/core/variable.py"
              },
              {
                "span_ids": [
                  "Coordinates",
                  "Coordinates.update"
                ],
                "file_path": "xarray/core/coordinates.py"
              },
              {
                "span_ids": [
                  "OrderedSet",
                  "OrderedSet.update"
                ],
                "file_path": "xarray/core/utils.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 6005,
          "output_tokens": 459,
          "retry_message": null,
          "completion_cost": 0.0249
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "relevant": true,
            "scratch_pad": "The provided file context contains relevant code to investigate the issues reported. Specifically, the `chunk` method in both `DataArray` and `Dataset` classes is identified as important for ensuring chunking is maintained after updates and constructions. The `update` method in the `Dataset` class is also relevant, as it provides the functionality for updating a dataset. However, the specific behavior of these methods when handling chunked dask DataArrays is not fully detailed in the file context provided. Therefore, while the relevant areas of the codebase are identified, the exact implementation details that might cause eager evaluation of chunked dask DataArrays are not explicitly provided.",
            "search_suggestions": "Further investigation into the implementation of the `chunk` and `update` methods, especially how they interact with dask arrays, is suggested to understand the exact cause of the eager evaluation behavior reported in the issues."
          },
          "output": {
            "message": "Further investigation into the implementation of the `chunk` and `update` methods, especially how they interact with dask arrays, is suggested to understand the exact cause of the eager evaluation behavior reported in the issues."
          },
          "input_tokens": 6826,
          "output_tokens": 212,
          "retry_message": null,
          "completion_cost": 0.023658
        }
      ]
    },
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": "Further investigation into the implementation of the `chunk` and `update` methods, especially how they interact with dask arrays, is suggested to understand the exact cause of the eager evaluation behavior reported in the issues.",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The provided context clarifies the implementation of the `chunk` and `update` methods within the `DataArray` and `Dataset` classes. Further investigation is needed to understand why the chunked dask DataArray is losing its chunking properties after the `DataSet.update` and Dataset constructor operations. The `chunk` method in `DataArray` converts the DataArray to a temporary Dataset and applies chunking there before converting back, while the `update` method in `Dataset` uses the `dataset_update_method` to merge results. It is crucial to ensure that these methods maintain the chunked state of dask arrays, potentially by adding a check or modifying the chunk handling within these methods.",
            "search_requests": [
              {
                "query": "How does the chunk method in DataArray and Dataset handle dask arrays to maintain chunking?",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": null,
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 466,
                "span_id": "Dataset.chunk",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 48,
                "tokens": 170,
                "span_id": "Dataset.update",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 1,
                "tokens": 409,
                "span_id": "DataArray.chunk",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 4,
                "tokens": 634,
                "span_id": "Variable.chunk",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 157,
                "tokens": 37,
                "span_id": "IndexVariable.chunk",
                "file_path": "xarray/core/variable.py"
              },
              {
                "rank": 55,
                "tokens": 62,
                "span_id": "Coordinates.update",
                "file_path": "xarray/core/coordinates.py"
              }
            ],
            "search_requests": [
              {
                "query": "How does the chunk method in DataArray and Dataset handle dask arrays to maintain chunking?",
                "class_names": [],
                "code_snippet": null,
                "file_pattern": null,
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "input_tokens": 9856,
          "output_tokens": 254,
          "retry_message": null,
          "completion_cost": 0.033378
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 466,
            "span_id": "Dataset.chunk",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 48,
            "tokens": 170,
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 1,
            "tokens": 409,
            "span_id": "DataArray.chunk",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 4,
            "tokens": 634,
            "span_id": "Variable.chunk",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 157,
            "tokens": 37,
            "span_id": "IndexVariable.chunk",
            "file_path": "xarray/core/variable.py"
          },
          {
            "rank": 55,
            "tokens": 62,
            "span_id": "Coordinates.update",
            "file_path": "xarray/core/coordinates.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "The reported issue indicates two main problems related to chunked dask DataArrays losing their chunking after certain operations in xarray. Specifically, using `DataSet.update` causes the DataArray to evaluate its values eagerly, and the chunking is lost. Additionally, creating a Dataset with a DataArray seems to trigger computation of the dask array, converting it from a dask array to a numpy array.\n\nUpon analyzing the provided search results and file context, relevant code spans are found in the `Dataset` and `DataArray` classes, particularly in the `update` and `chunk` methods. These methods are responsible for handling the chunking and updating of DataArrays within Datasets. The issue likely arises due to the implementation of these methods, possibly due to how the data is being merged or updated without preserving the chunking information of the dask arrays.\n\nThe search results confirm the presence of these methods in the `Dataset` and `DataArray` classes, which aligns with the context provided. However, the detailed implementation in the `update` method of the `Dataset` class and its interaction with the underlying dask arrays during the update process might need closer inspection to identify exactly where the issue arises with losing chunking information.",
            "identified_spans": [
              {
                "span_ids": [
                  "Dataset",
                  "Dataset.update"
                ],
                "file_path": "xarray/core/dataset.py"
              },
              {
                "span_ids": [
                  "DataArray",
                  "DataArray.chunk"
                ],
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "span_ids": [
                  "Variable",
                  "Variable.chunk"
                ],
                "file_path": "xarray/core/variable.py"
              },
              {
                "span_ids": [
                  "Coordinates",
                  "Coordinates.update"
                ],
                "file_path": "xarray/core/coordinates.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 10555,
          "output_tokens": 438,
          "retry_message": null,
          "completion_cost": 0.038235
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": true,
            "relevant": true,
            "scratch_pad": "The provided context clarifies the implementation of the `chunk` and `update` methods within the `DataArray` and `Dataset` classes. Further investigation is needed to understand why the chunked dask DataArray is losing its chunking properties after the `DataSet.update` and Dataset constructor operations. The `chunk` method in `DataArray` converts the DataArray to a temporary Dataset and applies chunking there before converting back, while the `update` method in `Dataset` uses the `dataset_update_method` to merge results. It is crucial to ensure that these methods maintain the chunked state of dask arrays, potentially by adding a check or modifying the chunk handling within these methods.",
            "search_suggestions": null
          },
          "output": {},
          "input_tokens": 6845,
          "output_tokens": 182,
          "retry_message": null,
          "completion_cost": 0.023265
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The issue reported involves the eager evaluation of chunked dask DataArray when using `DataSet.update`. This causes the DataArray to lose its chunked status. To resolve this, we need to ensure that the update operation preserves the chunked status of the DataArray. The relevant files and methods to focus on are `xarray/core/dataset.py` and `xarray/core/variable.py`, particularly the `chunk` methods and the `update` method in `Dataset`.",
            "instructions": "Update the `update` method in `Dataset` to ensure that it preserves the chunked status of the DataArray when updating. This could involve checking if the DataArray is chunked before and after the update operation, and if it is not chunked after the update, reapply the chunking."
          },
          "output": {
            "span_id": "Dataset.update",
            "end_line": 3910,
            "file_path": "xarray/core/dataset.py",
            "start_line": 3883,
            "instructions": "Update the `update` method in `Dataset` to ensure that it preserves the chunked status of the DataArray when updating. This could involve checking if the DataArray is chunked before and after the update operation, and if it is not chunked after the update, reapply the chunking."
          },
          "input_tokens": 7199,
          "output_tokens": 235,
          "retry_message": null,
          "completion_cost": 0.025122000000000002
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "Dataset.update",
        "end_line": 3910,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": 3883,
        "temperature": 1.6,
        "instructions": "Update the `update` method in `Dataset` to ensure that it preserves the chunked status of the DataArray when updating. This could involve checking if the DataArray is chunked before and after the update operation, and if it is not chunked after the update, reapply the chunking.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "```diff\n<replace>\n    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n        \"\"\"Update this dataset's variables with those from another dataset.\n\n        Parameters\n        ----------\n        other : Dataset or mapping\n            Variables with which to update this dataset. One of:\n\n            - Dataset\n            - mapping {var name: DataArray}\n            - mapping {var name: Variable}\n            - mapping {var name: (dimension name, array-like)}\n            - mapping {var name: (tuple of dimension names, array-like)}\n\n\n        Returns\n        -------\n        updated : Dataset\n            Updated dataset.\n\n        Raises\n        ------\n        ValueError\n            If any dimensions would have inconsistent sizes in the updated\n            dataset.\n        \"\"\"\n        merge_result = dataset_update_method(self, other)\n        updated_ds = self._replace(inplace=True, **merge_result._asdict())\n+       for var_name, var in updated_ds.variables.items():\n+           if isinstance(var, Variable) and not is_duck_dask_array(var._data):\n+               updated_ds[var_name] = var.chunk({dim: -1 for dim in var.dims})\n        return updated_ds\n</replace>"
          },
          "output": null,
          "input_tokens": 11524,
          "output_tokens": 294,
          "retry_message": "The following diff was not applied:\n --- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -3907,7 +3907,11 @@\n             dataset.\n         \"\"\"\n         merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+        updated_ds = self._replace(inplace=True, **merge_result._asdict())\n++       for var_name, var in updated_ds.variables.items():\n++           if isinstance(var, Variable) and not is_duck_dask_array(var._data):\n++               updated_ds[var_name] = var.chunk({dim: -1 for dim in var.dims})\n+        return updated_ds\n \n     def merge(\n         self,\n. \nErrors:\nThe updated code is invalid.\nMake sure that you return the unchanged code in the replace tag exactly as it is. If you want to reject the instructions, use the reject function.",
          "completion_cost": 0.038981999999999996
        },
        {
          "action": {
            "content": "<reject>\nThe provided solution attempts to modify the variables within the Dataset after the update operation, but the changes are not correctly applied due to the syntax errors in the diff. Additionally, the initial response may not effectively reject the instructions.\n</reject>"
          },
          "output": {
            "message": "\nThe provided solution attempts to modify the variables within the Dataset after the update operation, but the changes are not correctly applied due to the syntax errors in the diff. Additionally, the initial response may not effectively reject the instructions.\n"
          },
          "input_tokens": 12070,
          "output_tokens": 52,
          "retry_message": null,
          "completion_cost": 0.03699
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": "\nThe provided solution attempts to modify the variables within the Dataset after the update operation, but the changes are not correctly applied due to the syntax errors in the diff. Additionally, the initial response may not effectively reject the instructions.\n",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "reject",
            "finish": null,
            "reject": "The initial solution contained syntax errors. We need to ensure that the chunked status of the DataArray is preserved during the update operation, which requires careful modification of the `update` method in the `Dataset` class. The proposed change needs to be revised to avoid syntax errors and properly maintain the chunked status of DataArrays.",
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The issue involves the loss of chunked status for dask DataArray after using `DataSet.update`. The initial solution attempt had syntax errors. To address this, we need to ensure that the chunked status of the DataArray is preserved during the update operation. We will focus on the `update` method in the `Dataset` class, ensuring that the chunking information is maintained when variables are updated.",
            "instructions": null
          },
          "output": {
            "message": "The initial solution contained syntax errors. We need to ensure that the chunked status of the DataArray is preserved during the update operation, which requires careful modification of the `update` method in the `Dataset` class. The proposed change needs to be revised to avoid syntax errors and properly maintain the chunked status of DataArrays."
          },
          "input_tokens": 7486,
          "output_tokens": 220,
          "retry_message": null,
          "completion_cost": 0.025758000000000003
        }
      ]
    },
    {
      "name": "Rejected",
      "state": {
        "model": "gpt-4o",
        "message": "The initial solution contained syntax errors. We need to ensure that the chunked status of the DataArray is preserved during the update operation, which requires careful modification of the `update` method in the `Dataset` class. The proposed change needs to be revised to avoid syntax errors and properly maintain the chunked status of DataArrays.",
        "max_tokens": 1000,
        "temperature": 0.0,
        "max_iterations": null,
        "include_message_history": false
      },
      "actions": []
    }
  ],
  "initial_message": "DataSet.update causes chunked dask DataArray to evalute its values eagerly \n**What happened**:\r\nUsed `DataSet.update` to update a chunked dask DataArray, but the DataArray is no longer chunked after the update.\r\n\r\n**What you expected to happen**:\r\nThe chunked DataArray should still be chunked after the update\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nfoo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()  # foo is chunked\r\nds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})  # foo is still chunked here\r\nds  # you can verify that foo is chunked\r\n```\r\n```python\r\nupdate_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\r\nupdate_dict[\"foo\"][1]  # foo is still chunked\r\n```\r\n```python\r\nds.update(update_dict)\r\nds  # now foo is no longer chunked\r\n```\r\n\r\n**Environment**:\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\n```\r\ncommit: None\r\npython: 3.8.3 (default, Jul  2 2020, 11:26:31) \r\n[Clang 10.0.0 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 19.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\nlibhdf5: 1.10.6\r\nlibnetcdf: None\r\n\r\nxarray: 0.16.0\r\npandas: 1.0.5\r\nnumpy: 1.18.5\r\nscipy: 1.5.0\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: None\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.20.0\r\ndistributed: 2.20.0\r\nmatplotlib: 3.2.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\npint: None\r\nsetuptools: 49.2.0.post20200714\r\npip: 20.1.1\r\nconda: None\r\npytest: 5.4.3\r\nIPython: 7.16.1\r\nsphinx: None\r\n```\r\n\r\n</details>\nDataset constructor with DataArray triggers computation\nIs it intentional that creating a Dataset with a DataArray and dimension names for a single variable causes computation of that variable?  In other words, why does ```xr.Dataset(dict(a=('d0', xr.DataArray(da.random.random(10)))))``` cause the dask array to compute?\r\n\r\nA longer example:\r\n\r\n```python\r\nimport dask.array as da\r\nimport xarray as xr\r\nx = da.random.randint(1, 10, size=(100, 25))\r\nds = xr.Dataset(dict(a=xr.DataArray(x, dims=('x', 'y'))))\r\ntype(ds.a.data)\r\ndask.array.core.Array\r\n\r\n# Recreate the dataset with the same array, but also redefine the dimensions\r\nds2 = xr.Dataset(dict(a=(('x', 'y'), ds.a))\r\ntype(ds2.a.data)\r\nnumpy.ndarray\r\n```\r\n\r\n\n"
}