{
  "info": {
    "error": "Traceback (most recent call last):\n  File \"/workspace/moatless/moatless-tools/driver/run_moatless.py\", line 89, in run_moatless_core\n    response = loop.run(problem_statement)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/moatless/moatless-tools/moatless/loop.py\", line 179, in run\n    self._run()\n  File \"/workspace/moatless/moatless-tools/moatless/loop.py\", line 381, in _run\n    response = self.state.handle_action(action)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/moatless/moatless-tools/moatless/edit/clarify.py\", line 100, in handle_action\n    start_line, end_line = self.get_line_span(\n                           ^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/moatless/moatless-tools/moatless/edit/clarify.py\", line 250, in get_line_span\n    end_line = _get_post_end_line_index(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/moatless/moatless-tools/moatless/edit/clarify.py\", line 308, in _get_post_end_line_index\n    raise IndexError(\"end_line is out of range.\")\nIndexError: end_line is out of range.\n",
    "duration": 207.12783527374268,
    "submission": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..760b3371 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3907,7 +3907,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             dataset.\n         \"\"\"\n         merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+        updated_dataset = self._replace(inplace=True, **merge_result._asdict())\n+\n+        # Ensure chunked status is preserved after update\n+        for var_name, var in updated_dataset.variables.items():\n+            if var_name in self.variables and isinstance(self.variables[var_name].data, da.Array) and not isinstance(var.data, da.Array):\n+                updated_dataset[var_name] = var.chunk(self.variables[var_name].data.chunks)\n+\n+        return updated_dataset\n \n     def merge(\n         self,\n",
    "total_cost": 0.191259,
    "instance_id": "pydata__xarray-4493",
    "evaluation_name": "run_on_remaining_250_for_july_4_t_1.6"
  },
  "name": "AgenticLoop",
  "transitions": [
    {
      "name": "SearchCode",
      "state": {
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": null,
        "max_search_results": 25,
        "support_test_files": false,
        "initial_context_tokens": 4000,
        "initial_search_results": 50,
        "include_message_history": true,
        "provide_initial_context": true,
        "initial_context_spans_per_file": 5,
        "max_retries_with_any_file_context": 3
      },
      "actions": [
        {
          "action": {
            "complete": false,
            "scratch_pad": "The issue involves the behavior of chunked dask DataArrays when using the `DataSet.update` method and the creation of a Dataset with a DataArray. For the first issue, focus on methods related to chunking and update in the Dataset class. For the second issue, investigate how DataArrays are handled during Dataset creation, specifically looking at the chunking behavior and any potential triggers for computation. Additionally, examine the handling of dimensions and how they might affect the chunking and computation of DataArrays within Datasets.",
            "search_requests": [
              {
                "query": null,
                "class_names": [
                  "Dataset",
                  "DataArray"
                ],
                "code_snippet": null,
                "file_pattern": "**/core/**/*.py",
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "output": {
            "complete": false,
            "ranked_spans": [
              {
                "rank": 0,
                "tokens": 409,
                "span_id": "DataArray.chunk",
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "rank": 1,
                "tokens": 466,
                "span_id": "Dataset.chunk",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "rank": 1,
                "tokens": 170,
                "span_id": "Dataset.update",
                "file_path": "xarray/core/dataset.py"
              }
            ],
            "search_requests": [
              {
                "query": null,
                "class_names": [
                  "Dataset",
                  "DataArray"
                ],
                "code_snippet": null,
                "file_pattern": "**/core/**/*.py",
                "function_names": [
                  "chunk",
                  "update"
                ]
              }
            ]
          },
          "input_tokens": 4646,
          "output_tokens": 184,
          "retry_message": null,
          "completion_cost": 0.016698
        }
      ]
    },
    {
      "name": "IdentifyCode",
      "state": {
        "model": "deepseek-coder",
        "query": null,
        "class_name": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "code_snippet": null,
        "file_pattern": null,
        "ranked_spans": [
          {
            "rank": 0,
            "tokens": 409,
            "span_id": "DataArray.chunk",
            "file_path": "xarray/core/dataarray.py"
          },
          {
            "rank": 1,
            "tokens": 466,
            "span_id": "Dataset.chunk",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "rank": 1,
            "tokens": 170,
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py"
          }
        ],
        "function_name": null,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false
      },
      "actions": [
        {
          "action": {
            "scratch_pad": "The reported issue mentions that using `DataSet.update` causes a chunked dask DataArray to evaluate its values eagerly and lose its chunking. This suggests that the problem lies within the `update` method of the Dataset class or the `chunk` method of the DataArray class, as these are the main methods involved in the reported issue.\n\nThe provided search results contain the relevant code spans from `xarray/core/dataarray.py` and `xarray/core/dataset.py`. In `xarray/core/dataarray.py`, the `chunk` method of the DataArray class is identified. This method is responsible for chunking the DataArray, which is crucial for handling large datasets and parallel computing.\n\nIn `xarray/core/dataset.py`, the `chunk` method of the Dataset class and the `update` method are identified. The `chunk` method in the Dataset class manages chunking for the entire dataset, while the `update` method is responsible for updating the Dataset with new data. The issue might be related to how these methods interact, particularly when the `update` method is called on a Dataset containing chunked DataArrays.\n\nBy analyzing these methods, we can pinpoint the potential source of the problem. The `chunk` method in both DataArray and Dataset classes is involved in maintaining or changing chunking, and the `update` method in the Dataset class is where the problem arises. Therefore, these methods are the primary focus for addressing the reported issue.",
            "identified_spans": [
              {
                "span_ids": [
                  "DataArray.__init__",
                  "DataArray.chunk"
                ],
                "file_path": "xarray/core/dataarray.py"
              },
              {
                "span_ids": [
                  "Dataset.chunk",
                  "Dataset.update"
                ],
                "file_path": "xarray/core/dataset.py"
              }
            ]
          },
          "output": {},
          "input_tokens": 4871,
          "output_tokens": 443,
          "retry_message": null,
          "completion_cost": 0.021258
        }
      ]
    },
    {
      "name": "DecideRelevance",
      "state": {
        "model": "deepseek-coder",
        "max_tokens": 1000,
        "temperature": 1.6,
        "expand_context": true,
        "max_iterations": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": false,
        "finish_after_relevant_count": 2
      },
      "actions": [
        {
          "action": {
            "complete": true,
            "relevant": true,
            "scratch_pad": "The file context includes relevant code sections from xarray/core/dataarray.py and xarray/core/dataset.py. Specifically, it contains methods and properties related to chunking and update mechanisms, which are critical for addressing both reported issues. For the first issue regarding `DataSet.update` causing eager evaluation of chunked dask DataArray, the relevant code includes the `chunk` method in both DataArray and Dataset classes, as well as the `update` method in the Dataset class. These methods directly relate to the manipulation and update of DataArrays and Datasets, which could affect chunking behavior. For the second issue about the Dataset constructor causing computation of DataArray, the file context includes the `__init__` method of DataArray and Dataset, which deals with the creation and modification of DataArrays and Datasets. Additionally, methods like `_to_temp_dataset` and `_from_temp_dataset` in DataArray, and `_replace` in Dataset, are relevant as they handle internal state and transformations that could influence computation and chunking behavior. Overall, the provided file context seems sufficient to understand and potentially debug the issues mentioned, as it covers data handling, chunking, and update mechanisms within the Dataset and DataArray classes.",
            "search_suggestions": null
          },
          "output": {},
          "input_tokens": 4802,
          "output_tokens": 297,
          "retry_message": null,
          "completion_cost": 0.018861
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": null,
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "Dataset.update",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The issue is that when `DataSet.update` is used to update a chunked dask DataArray, the DataArray loses its chunked status. This is problematic because it defeats the purpose of using dask for lazy computation. The relevant files to update are `dataset.py` and `dataarray.py`. The primary function to examine is `Dataset.update`, which calls `dataset_update_method`. We need to ensure that any updates to the DataArray preserve its chunked status.",
            "instructions": "Update the `Dataset.update` method to ensure that any updates to the DataArray preserve its chunked status. Specifically, when updating a DataArray within a Dataset, the method should check if the DataArray is chunked and ensure that it remains chunked after the update."
          },
          "output": {
            "span_id": "Dataset.update",
            "end_line": 3910,
            "file_path": "xarray/core/dataset.py",
            "start_line": 3883,
            "instructions": "Update the `Dataset.update` method to ensure that any updates to the DataArray preserve its chunked status. Specifically, when updating a DataArray within a Dataset, the method should check if the DataArray is chunked and ensure that it remains chunked after the update."
          },
          "input_tokens": 5081,
          "output_tokens": 217,
          "retry_message": null,
          "completion_cost": 0.018498
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "Dataset.update",
        "end_line": 3910,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": 3883,
        "temperature": 1.6,
        "instructions": "Update the `Dataset.update` method to ensure that any updates to the DataArray preserve its chunked status. Specifically, when updating a DataArray within a Dataset, the method should check if the DataArray is chunked and ensure that it remains chunked after the update.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "```python\n<replace>\n    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n        \"\"\"Update this dataset's variables with those from another dataset.\n\n        Parameters\n        ----------\n        other : Dataset or mapping\n            Variables with which to update this dataset. One of:\n\n            - Dataset\n            - mapping {var name: DataArray}\n            - mapping {var name: Variable}\n            - mapping {var name: (dimension name, array-like)}\n            - mapping {var name: (tuple of dimension names, array-like)}\n\n\n        Returns\n        -------\n        updated : Dataset\n            Updated dataset.\n\n        Raises\n        ------\n        ValueError\n            If any dimensions would have inconsistent sizes in the updated\n            dataset.\n        \"\"\"\n        merge_result = dataset_update_method(self, other)\n        updated_dataset = self._replace(inplace=True, **merge_result._asdict())\n        \n        # Ensure chunked status is preserved after update\n        for var_name, var in updated_dataset.variables.items():\n            if var_name in self.variables and isinstance(self.variables[var_name].data, da.Array) and not isinstance(var.data, da.Array):\n                updated_dataset[var_name] = var.chunk(self.variables[var_name].data.chunks)\n        \n        return updated_dataset\n</replace>"
          },
          "output": {
            "diff": "--- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -3907,7 +3907,14 @@\n             dataset.\n         \"\"\"\n         merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+        updated_dataset = self._replace(inplace=True, **merge_result._asdict())\n+        \n+        # Ensure chunked status is preserved after update\n+        for var_name, var in updated_dataset.variables.items():\n+            if var_name in self.variables and isinstance(self.variables[var_name].data, da.Array) and not isinstance(var.data, da.Array):\n+                updated_dataset[var_name] = var.chunk(self.variables[var_name].data.chunks)\n+        \n+        return updated_dataset\n \n     def merge(\n         self,\n",
            "message": "Applied the change to xarray/core/dataset.py.\n\n<scratch_pad>\n```python\n</scratch_pad>",
            "verification_errors": [
              {
                "code": "E0602",
                "line": 3914,
                "message": "Undefined variable 'da'",
                "file_path": "xarray/core/dataset.py"
              },
              {
                "code": "E0602",
                "line": 3914,
                "message": "Undefined variable 'da'",
                "file_path": "xarray/core/dataset.py"
              }
            ]
          },
          "input_tokens": 7065,
          "output_tokens": 322,
          "retry_message": null,
          "completion_cost": 0.026025000000000003
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": "--- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -3907,7 +3907,14 @@\n             dataset.\n         \"\"\"\n         merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+        updated_dataset = self._replace(inplace=True, **merge_result._asdict())\n+        \n+        # Ensure chunked status is preserved after update\n+        for var_name, var in updated_dataset.variables.items():\n+            if var_name in self.variables and isinstance(self.variables[var_name].data, da.Array) and not isinstance(var.data, da.Array):\n+                updated_dataset[var_name] = var.chunk(self.variables[var_name].data.chunks)\n+        \n+        return updated_dataset\n \n     def merge(\n         self,\n",
        "model": "deepseek-coder",
        "message": "Applied the change to xarray/core/dataset.py.\n\n<scratch_pad>\n```python\n</scratch_pad>",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": [
          {
            "code": "E0602",
            "line": 3914,
            "message": "Undefined variable 'da'",
            "file_path": "xarray/core/dataset.py"
          },
          {
            "code": "E0602",
            "line": 3914,
            "message": "Undefined variable 'da'",
            "file_path": "xarray/core/dataset.py"
          }
        ],
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "imports",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The lint error is due to the undefined variable 'da' in the `Dataset.update` method. This variable should be imported from dask.array.",
            "instructions": "Import the 'da' variable from dask.array at the beginning of the file to resolve the undefined variable issue."
          },
          "output": {
            "span_id": "imports",
            "file_path": "xarray/core/dataset.py",
            "instructions": "Import the 'da' variable from dask.array at the beginning of the file to resolve the undefined variable issue."
          },
          "input_tokens": 5745,
          "output_tokens": 127,
          "retry_message": null,
          "completion_cost": 0.01914
        }
      ]
    },
    {
      "name": "ClarifyCodeChange",
      "state": {
        "model": "deepseek-coder",
        "span_id": "imports",
        "end_line": null,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": null,
        "temperature": 1.6,
        "instructions": "Import the 'da' variable from dask.array at the beginning of the file to resolve the undefined variable issue.\n\nThe import for 'da' from dask.array is missing at the beginning of the file where other imports are listed. This can be added near the start to ensure 'da' is defined when used later in the code.",
        "max_iterations": null,
        "include_message_history": false,
        "max_tokens_in_edit_prompt": 500
      },
      "actions": [
        {
          "action": {
            "reject": false,
            "end_line": 53,
            "start_line": 53,
            "scratch_pad": "The import for 'da' from dask.array is missing at the beginning of the file where other imports are listed. This can be added near the start to ensure 'da' is defined when used later in the code."
          },
          "output": {
            "span_id": "imports",
            "end_line": 57,
            "file_path": "xarray/core/dataset.py",
            "start_line": 49,
            "instructions": "Import the 'da' variable from dask.array at the beginning of the file to resolve the undefined variable issue.\n\nThe import for 'da' from dask.array is missing at the beginning of the file where other imports are listed. This can be added near the start to ensure 'da' is defined when used later in the code."
          },
          "input_tokens": 1524,
          "output_tokens": 90,
          "retry_message": null,
          "completion_cost": 0.005922
        }
      ]
    },
    {
      "name": "EditCode",
      "state": {
        "model": "deepseek-coder",
        "verify": true,
        "span_id": "imports",
        "end_line": 57,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": 49,
        "temperature": 1.6,
        "instructions": "Import the 'da' variable from dask.array at the beginning of the file to resolve the undefined variable issue.\n\nThe import for 'da' from dask.array is missing at the beginning of the file where other imports are listed. This can be added near the start to ensure 'da' is defined when used later in the code.",
        "max_iterations": 8,
        "chain_of_thought": false,
        "show_file_context": true,
        "show_initial_message": true,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true
      },
      "actions": [
        {
          "action": {
            "content": "<replace>\nfrom . import (\n    resample,\n    rolling,\n    utils,\n    weighted,\n    dask as da,  # Import 'da' from dask.array\n)\nfrom .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align\nfrom .common import (\n    DataWithCoords,\n    ImplementsDatasetReduce,\n</replace>"
          },
          "output": null,
          "input_tokens": 7042,
          "output_tokens": 91,
          "retry_message": "The following diff was not applied:\n --- xarray/core/dataset.py\n+++ xarray/core/dataset.py\n@@ -46,10 +46,12 @@\n     formatting_html,\n     groupby,\n     ops,\n+from . import (\n     resample,\n     rolling,\n     utils,\n     weighted,\n+    dask as da,  # Import 'da' from dask.array\n )\n from .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align\n from .common import (\n. \nErrors:\nThe updated code is invalid.\nMake sure that you return the unchanged code in the replace tag exactly as it is. If you want to reject the instructions, use the reject function.",
          "completion_cost": 0.022491
        },
        {
          "action": {
            "content": "<reject>\nThe updated code attempted to modify the existing structure by inserting an import statement within an existing import statement, which is invalid syntax in Python.\n</reject>"
          },
          "output": {
            "message": "\nThe updated code attempted to modify the existing structure by inserting an import statement within an existing import statement, which is invalid syntax in Python.\n"
          },
          "input_tokens": 7315,
          "output_tokens": 35,
          "retry_message": null,
          "completion_cost": 0.02247
        }
      ]
    },
    {
      "name": "PlanToCode",
      "state": {
        "diff": null,
        "model": "deepseek-coder",
        "message": "\nThe updated code attempted to modify the existing structure by inserting an import statement within an existing import statement, which is invalid syntax in Python.\n",
        "max_tokens": 1000,
        "temperature": 1.6,
        "max_iterations": 8,
        "finish_on_review": false,
        "verification_errors": null,
        "max_prompt_file_tokens": 4000,
        "include_message_history": true,
        "allow_hallucinated_spans": false,
        "max_tokens_in_edit_prompt": 500,
        "expand_context_with_related_spans": true
      },
      "actions": [
        {
          "action": {
            "action": "modify",
            "finish": null,
            "reject": null,
            "span_id": "imports",
            "file_path": "xarray/core/dataset.py",
            "scratch_pad": "The lint error is due to the lack of an import for 'da' from dask.array. We need to add this import at the beginning of the file in the appropriate block of existing imports to ensure the code runs without syntax error.",
            "instructions": "Add the import statement 'import dask.array as da' at the beginning of the file where other imports are located."
          },
          "output": {
            "span_id": "imports",
            "file_path": "xarray/core/dataset.py",
            "instructions": "Add the import statement 'import dask.array as da' at the beginning of the file where other imports are located."
          },
          "input_tokens": 5907,
          "output_tokens": 145,
          "retry_message": null,
          "completion_cost": 0.019896
        }
      ]
    },
    {
      "name": "ClarifyCodeChange",
      "state": {
        "model": "deepseek-coder",
        "span_id": "imports",
        "end_line": null,
        "file_path": "xarray/core/dataset.py",
        "max_tokens": 1000,
        "start_line": null,
        "temperature": 1.6,
        "instructions": "Add the import statement 'import dask.array as da' at the beginning of the file where other imports are located.",
        "max_iterations": null,
        "include_message_history": false,
        "max_tokens_in_edit_prompt": 500
      },
      "actions": []
    }
  ],
  "initial_message": "DataSet.update causes chunked dask DataArray to evalute its values eagerly \n**What happened**:\r\nUsed `DataSet.update` to update a chunked dask DataArray, but the DataArray is no longer chunked after the update.\r\n\r\n**What you expected to happen**:\r\nThe chunked DataArray should still be chunked after the update\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nfoo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()  # foo is chunked\r\nds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})  # foo is still chunked here\r\nds  # you can verify that foo is chunked\r\n```\r\n```python\r\nupdate_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\r\nupdate_dict[\"foo\"][1]  # foo is still chunked\r\n```\r\n```python\r\nds.update(update_dict)\r\nds  # now foo is no longer chunked\r\n```\r\n\r\n**Environment**:\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\n```\r\ncommit: None\r\npython: 3.8.3 (default, Jul  2 2020, 11:26:31) \r\n[Clang 10.0.0 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 19.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\nlibhdf5: 1.10.6\r\nlibnetcdf: None\r\n\r\nxarray: 0.16.0\r\npandas: 1.0.5\r\nnumpy: 1.18.5\r\nscipy: 1.5.0\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: None\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.20.0\r\ndistributed: 2.20.0\r\nmatplotlib: 3.2.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\npint: None\r\nsetuptools: 49.2.0.post20200714\r\npip: 20.1.1\r\nconda: None\r\npytest: 5.4.3\r\nIPython: 7.16.1\r\nsphinx: None\r\n```\r\n\r\n</details>\nDataset constructor with DataArray triggers computation\nIs it intentional that creating a Dataset with a DataArray and dimension names for a single variable causes computation of that variable?  In other words, why does ```xr.Dataset(dict(a=('d0', xr.DataArray(da.random.random(10)))))``` cause the dask array to compute?\r\n\r\nA longer example:\r\n\r\n```python\r\nimport dask.array as da\r\nimport xarray as xr\r\nx = da.random.randint(1, 10, size=(100, 25))\r\nds = xr.Dataset(dict(a=xr.DataArray(x, dims=('x', 'y'))))\r\ntype(ds.a.data)\r\ndask.array.core.Array\r\n\r\n# Recreate the dataset with the same array, but also redefine the dimensions\r\nds2 = xr.Dataset(dict(a=(('x', 'y'), ds.a))\r\ntype(ds2.a.data)\r\nnumpy.ndarray\r\n```\r\n\r\n\n"
}